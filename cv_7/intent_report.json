{
  "canthelp": {
    "precision": 0.6666666666666666,
    "recall": 0.7692307692307693,
    "f1-score": 0.7142857142857142,
    "support": 26,
    "confused_with": {
      "affirm": 3,
      "bye": 2
    }
  },
  "next_step": {
    "precision": 1.0,
    "recall": 0.9333333333333333,
    "f1-score": 0.9655172413793104,
    "support": 15,
    "confused_with": {
      "affirm": 1
    }
  },
  "user_wants_to_check_attendance": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 9,
    "confused_with": {}
  },
  "mood_unhappy": {
    "precision": 0.673469387755102,
    "recall": 0.5892857142857143,
    "f1-score": 0.6285714285714286,
    "support": 56,
    "confused_with": {
      "deny": 7,
      "react_positive": 5
    }
  },
  "bye": {
    "precision": 0.8571428571428571,
    "recall": 0.7346938775510204,
    "f1-score": 0.7912087912087913,
    "support": 49,
    "confused_with": {
      "affirm": 7,
      "react_positive": 2
    }
  },
  "mood_great": {
    "precision": 0.21739130434782608,
    "recall": 0.35714285714285715,
    "f1-score": 0.27027027027027023,
    "support": 14,
    "confused_with": {
      "affirm": 4,
      "react_positive": 2
    }
  },
  "user_submitted_usn": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 14,
    "confused_with": {}
  },
  "user_submitted_password": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 14,
    "confused_with": {
      "affirm": 8,
      "deny": 3,
      "react_positive": 1
    }
  },
  "deny": {
    "precision": 0.83,
    "recall": 0.8383838383838383,
    "f1-score": 0.8341708542713568,
    "support": 99,
    "confused_with": {
      "affirm": 6,
      "mood_unhappy": 3
    }
  },
  "react_positive": {
    "precision": 0.7101449275362319,
    "recall": 0.7903225806451613,
    "f1-score": 0.7480916030534351,
    "support": 62,
    "confused_with": {
      "affirm": 5,
      "mood_unhappy": 3
    }
  },
  "thank": {
    "precision": 0.9024390243902439,
    "recall": 0.9487179487179487,
    "f1-score": 0.9249999999999999,
    "support": 39,
    "confused_with": {
      "react_positive": 1,
      "affirm": 1
    }
  },
  "greet": {
    "precision": 0.9280575539568345,
    "recall": 0.8775510204081632,
    "f1-score": 0.9020979020979022,
    "support": 147,
    "confused_with": {
      "affirm": 8,
      "mood_great": 3
    }
  },
  "explain": {
    "precision": 0.7058823529411765,
    "recall": 0.75,
    "f1-score": 0.7272727272727272,
    "support": 16,
    "confused_with": {
      "react_positive": 1,
      "question_about_clg_internships": 1
    }
  },
  "affirm": {
    "precision": 0.7974683544303798,
    "recall": 0.84,
    "f1-score": 0.8181818181818181,
    "support": 225,
    "confused_with": {
      "mood_great": 9,
      "react_positive": 6
    }
  },
  "user_wants_to_check_marks": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 9,
    "confused_with": {}
  },
  "question_about_clg_internships": {
    "precision": 0.7777777777777778,
    "recall": 0.875,
    "f1-score": 0.823529411764706,
    "support": 8,
    "confused_with": {
      "explain": 1
    }
  },
  "accuracy": 0.8054862842892768,
  "macro avg": {
    "precision": 0.7541525129340685,
    "recall": 0.7689788712311754,
    "f1-score": 0.7592623601473413,
    "support": 802
  },
  "weighted avg": {
    "precision": 0.8003196818138725,
    "recall": 0.8054862842892768,
    "f1-score": 0.8014597198716858,
    "support": 802
  }
}